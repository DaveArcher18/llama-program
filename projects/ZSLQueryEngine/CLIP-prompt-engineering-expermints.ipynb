{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install clip-by-openai > /dev/null","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:08:54.281404Z","iopub.execute_input":"2021-07-06T13:08:54.281790Z","iopub.status.idle":"2021-07-06T13:10:14.666447Z","shell.execute_reply.started":"2021-07-06T13:08:54.281713Z","shell.execute_reply":"2021-07-06T13:10:14.665173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Imports ##########\n\nimport os \nimport datetime \nimport time\n\nfrom PIL import Image\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision\nimport torchmetrics\nimport pytorch_lightning as pl\n\nimport clip\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nif torch.cuda.is_available():  \n    print('Wohooo, GPU found!!')\n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"  \n    \ndevice = torch.device(dev)\n\ntorch.manual_seed(0)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:10:14.669277Z","iopub.execute_input":"2021-07-06T13:10:14.669953Z","iopub.status.idle":"2021-07-06T13:10:17.373100Z","shell.execute_reply.started":"2021-07-06T13:10:14.669905Z","shell.execute_reply":"2021-07-06T13:10:17.372253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Loading in CLIP","metadata":{}},{"cell_type":"code","source":"CLIP, preprocess = clip.load(\"RN50\", device=device, jit=False)\n#CLIP, preprocess = clip.load(\"ViT-B/32\", device=device)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:10:17.375160Z","iopub.execute_input":"2021-07-06T13:10:17.375537Z","iopub.status.idle":"2021-07-06T13:10:32.747948Z","shell.execute_reply.started":"2021-07-06T13:10:17.375495Z","shell.execute_reply":"2021-07-06T13:10:32.747119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Decoder ##########\n\ntokenizer_filepath = '../input/clip-backend-resources/bpe_simple_vocab_16e6.txt'\nimport gzip\nimport html\nimport os\nfrom functools import lru_cache\n\nimport ftfy\nimport regex as re\n\n@lru_cache()\ndef bytes_to_unicode():\n    \"\"\"\n    Returns list of utf-8 byte and a corresponding list of unicode strings.\n    The reversible bpe codes work on unicode strings.\n    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n    This is a signficant percentage of your normal, say, 32K bpe vocab.\n    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n    And avoids mapping to whitespace/control characters the bpe code barfs on.\n    \"\"\"\n    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n    cs = bs[:]\n    n = 0\n    for b in range(2**8):\n        if b not in bs:\n            bs.append(b)\n            cs.append(2**8+n)\n            n += 1\n    cs = [chr(n) for n in cs]\n    return dict(zip(bs, cs))\n\n\ndef get_pairs(word):\n    \"\"\"Return set of symbol pairs in a word.\n    Word is represented as tuple of symbols (symbols being variable-length strings).\n    \"\"\"\n    pairs = set()\n    prev_char = word[0]\n    for char in word[1:]:\n        pairs.add((prev_char, char))\n        prev_char = char\n    return pairs\n\n\ndef basic_clean(text):\n    text = ftfy.fix_text(text)\n    text = html.unescape(html.unescape(text))\n    return text.strip()\n            \n\n\ndef whitespace_clean(text):\n    text = re.sub(r'\\s+', ' ', text)\n    text = text.strip()\n    return text\n\n\n\nclass SimpleTokenizer(object):\n    def __init__(self, bpe_path: str = tokenizer_filepath):\n        self.byte_encoder = bytes_to_unicode()\n        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n        merges = open(bpe_path).read().split('\\n')\n        merges = merges[1:49152-256-2+1]\n        merges = [tuple(merge.split()) for merge in merges]\n        vocab = list(bytes_to_unicode().values())\n        vocab = vocab + [v+'</w>' for v in vocab]\n        for merge in merges:\n            vocab.append(''.join(merge))\n        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n        self.encoder = dict(zip(vocab, range(len(vocab))))\n        self.decoder = {v: k for k, v in self.encoder.items()}\n        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n        self.sot_token = self.encoder['<|startoftext|>']\n        self.eot_token = self.encoder['<|endoftext|>']\n        \n\n    def bpe(self, token):\n        if token in self.cache:\n            return self.cache[token]\n        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n        pairs = get_pairs(word)\n\n        if not pairs:\n            return token+'</w>'\n\n        while True:\n            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n            if bigram not in self.bpe_ranks:\n                break\n            first, second = bigram\n            new_word = []\n            i = 0\n            while i < len(word):\n                try:\n                    j = word.index(first, i)\n                    new_word.extend(word[i:j])\n                    i = j\n                except:\n                    new_word.extend(word[i:])\n                    break\n\n                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n                    new_word.append(first+second)\n                    i += 2\n                else:\n                    new_word.append(word[i])\n                    i += 1\n            new_word = tuple(new_word)\n            word = new_word\n            if len(word) == 1:\n                break\n            else:\n                pairs = get_pairs(word)\n        word = ' '.join(word)\n        self.cache[token] = word\n        return word\n    \n    def decode(self, tokens):\n        text = ''.join([self.decoder[token] for token in tokens])\n        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n        return text\n    \n    \n    def padded_decode(self, tokens):\n        \n        length = (tokens[0] == self.eot_token).nonzero(as_tuple=True)[0]\n \n        tokens = tokens[:,0:length][0][1:].cpu().numpy()\n                \n        text = self.decode(tokens)\n        return text.rstrip()\n        \n\ndecoder = SimpleTokenizer()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:10:32.749673Z","iopub.execute_input":"2021-07-06T13:10:32.749999Z","iopub.status.idle":"2021-07-06T13:10:32.965029Z","shell.execute_reply.started":"2021-07-06T13:10:32.749971Z","shell.execute_reply":"2021-07-06T13:10:32.964198Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# CLIP in a Pytorch Lightning Module","metadata":{}},{"cell_type":"code","source":"########## Making a PyTorch Lightning Module ##########\n\nclass CLIP_ZEROSHOT(pl.LightningModule):\n    def __init__(self, classes = None, CLIP = CLIP, decoder = decoder ):\n        super(CLIP_ZEROSHOT, self).__init__()\n        \n        self.CLIP = CLIP\n        \n        self.decoder = decoder\n        self.classes = classes\n        \n        if self.classes != None:\n            \n            self.acc = torchmetrics.Accuracy()\n            self.classes_dict = {}\n            for i, class_label in enumerate(classes):\n                self.classes_dict[decoder.padded_decode(class_label.unsqueeze(0))] = i  \n            self.test_cm = ConfusionMatrix(num_classes = len(self.classes)) #for plotting after testing epochs\n\n    \n    def encode_image(self, images):\n        with torch.no_grad():\n            image_features = self.CLIP.encode_image(images)\n            \n        return image_features     \n    \n    def encode_text(self, texts):\n        with torch.no_grad():\n            text_features = self.CLIP.encode_text(texts)\n            \n        return text_features  \n    \n    def forward(self, images):\n        with torch.no_grad():\n            image_features = self.CLIP.encode_image(images)\n            text_features = self.CLIP.encode_text(self.classes.cuda())\n\n            logits_per_image, logits_per_text = self.CLIP(images, self.classes.cuda())\n            probs = logits_per_image.softmax(dim=-1)\n        predicted_labels_indexes = torch.argmax(probs, dim = -1)    \n        return predicted_labels_indexes   \n    \n    def test_step(self, batch, batch_idx):\n        images = batch['image']\n        labels = batch['label']\n        \n        predicted_label_indexes = self.forward(images)\n        \n        predicted_labels = torch.zeros_like(labels).cuda()\n        \n        for i in range(len(predicted_label_indexes)):\n            predicted_labels[i] = self.classes[predicted_label_indexes[i]]\n\n        decoded_predicted_labels = torch.zeros(len(labels), dtype = torch.int)\n        decoded_labels = torch.zeros(len(labels), dtype = torch.int)\n        \n        for i in range(len(predicted_labels)):\n            decoded_predicted_labels[i] = self.classes_dict[self.decoder.padded_decode(predicted_labels[i])]\n            decoded_labels[i] = self.classes_dict[self.decoder.padded_decode(labels[i])]\n     \n    \n        test_acc = self.acc(decoded_predicted_labels, decoded_labels)\n        self.test_cm(decoded_predicted_labels.cuda(), decoded_labels.cuda()) #for plotting after testing epochs        \n        self.log('Accuracy', test_acc, prog_bar = True, on_epoch=True, sync_dist=True)\n        \n        \n        \nmodel = CLIP_ZEROSHOT()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:10:32.966446Z","iopub.execute_input":"2021-07-06T13:10:32.966798Z","iopub.status.idle":"2021-07-06T13:10:32.983006Z","shell.execute_reply.started":"2021-07-06T13:10:32.966761Z","shell.execute_reply":"2021-07-06T13:10:32.981853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prompt engineering","metadata":{}},{"cell_type":"code","source":"########## Prompt Engine Development Cell ##########\n\ndef control_engine(text):\n    return text\n    \n    \ndef basic_prompt_engine(text):\n    return f'{text}, a type of animal' \n\n\nfrom transformers import pipeline\n\nclassifier_zero_shot = pipeline(\"zero-shot-classification\")\n\ndef object_category_engine(text, classifier = classifier_zero_shot):\n    labels = ['animal', 'food', 'fruit', 'car', 'boat', 'airplane', 'appliance', 'electronic', 'accessory', 'furniture', 'kitchen', 'cutlery', 'crockery', 'person', 'fish', 'instrument', 'tool', 'sports equipment', 'vehicle', 'holy place', 'power tool']\n    out = classifier(text, labels)\n    category = out['labels'][np.argmax(out['scores'])]\n    \n    return f'{text}, a type of {category}'","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:10:32.984498Z","iopub.execute_input":"2021-07-06T13:10:32.984846Z","iopub.status.idle":"2021-07-06T13:11:35.749507Z","shell.execute_reply.started":"2021-07-06T13:10:32.984808Z","shell.execute_reply":"2021-07-06T13:11:35.748642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Experiments","metadata":{}},{"cell_type":"markdown","source":"## Plotting Functions","metadata":{}},{"cell_type":"code","source":"########## Plotting a similarity matrix ##########\n\n# Requires as many labels as images but can have more labels than images.\n\ndef plot_matrix(images, texts, model = model, decoder = decoder):\n    texts = texts.squeeze(1)\n    \n    image_features = model.encode_image(images.cuda())\n    \n    text_features = model.encode_text(texts.cuda())\n    \n    similarity = text_features.cpu().numpy() @ image_features.cpu().numpy().T\n    \n    count = len(texts)\n    \n    decoded_texts = [decoder.padded_decode(texts[i].unsqueeze(0)) for i in range(len(texts))]\n        \n    plt.figure(figsize=(20, 14))\n    plt.imshow(np.around(similarity).astype(int)) \n    \n    plt.yticks(range(count), decoded_texts, fontsize=18)\n    plt.xticks([])\n    \n    for i, image in enumerate(images):\n        plt.imshow(image.permute(1, 2, 0), extent=(i - 0.5, i + 0.5, -1.6, -0.6), origin=\"lower\")\n    \n    for x in range(similarity.shape[1]):\n        for y in range(similarity.shape[0]):\n            plt.text(x, y, f\"{similarity[y, x]:.2f}\", ha=\"center\", va=\"center\", size=12)\n\n    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n        plt.gca().spines[side].set_visible(False)\n\n    plt.xlim([-0.5, count - 0.5])\n    plt.ylim([count + 0.5, -2])\n\n    plt.title(\"Cosine similarity between text and image features\", size=20)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:35.752786Z","iopub.execute_input":"2021-07-06T13:11:35.753051Z","iopub.status.idle":"2021-07-06T13:11:35.763156Z","shell.execute_reply.started":"2021-07-06T13:11:35.753025Z","shell.execute_reply":"2021-07-06T13:11:35.762352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Plotting similarity between an image and multiple labels ##########\n\ndef plot_similarity_scores(images, texts, model = model, decoder = decoder, top_k = 5, padding = 15.0):\n    if len(texts) < top_k:\n        top_k = len(texts)\n    \n    texts = texts.squeeze(1)\n    decoded_texts = [decoder.padded_decode(texts[i].unsqueeze(0)) for i in range(len(texts))]\n    \n    image_features = model.encode_image(images.cuda())\n    \n    text_features = model.encode_text(texts.cuda())\n    \n    text_probs = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    top_probs, top_labels = text_probs.topk(top_k, dim=-1)\n    \n    top_probs = top_probs.cpu().numpy()\n    top_labels = top_labels.cpu().numpy()\n    \n    fig, ax = plt.subplots(len(images), 2, figsize=(15, 15))\n    fig.tight_layout(pad=padding)\n    \n    if len(images) > 1:\n        for i, image in enumerate(images):\n            ax[i][0].imshow(image.permute(1, 2, 0))\n\n            y = np.arange(top_probs.shape[-1])\n            ax[i][1].barh(y, top_probs[i])\n            plt.gca().invert_yaxis()\n            plt.gca().set_axisbelow(True)\n            ax[i][1].set_yticks(y)\n            ax[i][1].set_yticklabels([decoded_texts[index] for index in top_labels[i]])\n\n            ax[i][1].set_xlabel(\"probability\")\n\n        plt.show()\n    else:\n        for i, image in enumerate(images):\n            ax[0].imshow(image.permute(1, 2, 0))\n            y = np.arange(top_probs.shape[-1])[::-1]\n            ax[1].barh(y, top_probs[i])\n            ax[1].set_yticks(y)\n            ax[1].set_yticklabels([decoded_texts[index] for index in top_labels[i].numpy()])\n            ax[1].set_xlabel(\"probability\")\n            asp = np.diff(ax[1].get_xlim())[0] / np.diff(ax[1].get_ylim())[0]\n            ax[1].set_aspect(asp)\n\n        plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:35.766451Z","iopub.execute_input":"2021-07-06T13:11:35.766818Z","iopub.status.idle":"2021-07-06T13:11:35.785050Z","shell.execute_reply.started":"2021-07-06T13:11:35.766782Z","shell.execute_reply":"2021-07-06T13:11:35.784325Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"########## Functions to produce examples ##########\n\ndef image_example_producer(images):\n    # takes an array of images and returns a tensor containing the images\n    \n    for i in range(len(images)):\n        images[i] = preprocess(images[i])\n    \n    dims = tuple([i for i in images[0].shape])\n    dims = (len(images),) + dims\n    \n    \n    image_tensor = torch.zeros(dims)\n    for i in range(len(images)):\n        image_tensor[i] = images[i]\n        \n    return image_tensor\n    \n    \ndef text_example_producer(texts):\n    # takes an array of text labels and returns a tensor containing the tokenized text labels\n    \n    text_lengths = []\n    for i in range(len(texts)):    \n        texts[i] =  clip.tokenize(texts[i])\n    \n    dims = tuple([i for i in texts[0].shape])\n    dims = (len(texts),) + dims\n    \n    \n    texts_tensor = torch.zeros(dims, dtype=torch.long)\n    \n    for i in range(len(texts)):\n        texts_tensor[i] = texts[i]\n    \n    return texts_tensor\n        \n        \ndef read_image_folder(filepath):\n    paths = os.listdir(filepath)\n    images = []\n    for i in paths:\n        images.append(Image.open(os.path.join(filepath, i)))\n        \n    return images    \n    \ndef random_sample(list_of_images, n):\n    indexes = []\n    while len(indexes) < n:\n        rand = np.random.randint(0, len(list_of_images))\n        if rand not in indexes:\n            indexes.append(rand)\n        else:\n            continue\n        \n        \n    sample = []\n    for i in indexes:\n        sample.append(list_of_images[i])\n        \n    return sample    \n        \n        \nnp.random.seed(42)\n        ","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:35.786700Z","iopub.execute_input":"2021-07-06T13:11:35.787128Z","iopub.status.idle":"2021-07-06T13:11:35.799600Z","shell.execute_reply.started":"2021-07-06T13:11:35.787091Z","shell.execute_reply":"2021-07-06T13:11:35.798644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reading in random images and storing random noise texts¶\n","metadata":{}},{"cell_type":"code","source":"random_noise_images = read_image_folder('../input/small-clip-comparison-dataset/Noise and Random')\n\n\nrandom_noise_texts = ['Abstract Painting, a type of art', 'Bicycle, a type of vehicle', 'Chael Sonnen, a type of bad guy', 'Empire State Building, a type of building', 'Rubiks Cube, a type of toy', 'fhasfdgbeasd, a type of noise']\n","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:35.800938Z","iopub.execute_input":"2021-07-06T13:11:35.801434Z","iopub.status.idle":"2021-07-06T13:11:36.282969Z","shell.execute_reply.started":"2021-07-06T13:11:35.801383Z","shell.execute_reply":"2021-07-06T13:11:36.282093Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fruit Example","metadata":{}},{"cell_type":"code","source":"fruit_images = read_image_folder('../input/small-clip-comparison-dataset/Fruit')\n\nfruit_images = image_example_producer(fruit_images + random_sample(random_noise_images, 2))\n\n\nfruit_texts = ['Avocado', 'Avocado, a type of fruit', 'orange', 'Orange, a type of fruit', 'Apple', 'Apple, a type of fruit', 'Banana', 'Banana, a type of fruit']\nfruit_texts_tensor = text_example_producer(fruit_texts + random_noise_texts)\nplot_similarity_scores(fruit_images, fruit_texts_tensor, padding = 15)\nplot_matrix(fruit_images, fruit_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:36.284395Z","iopub.execute_input":"2021-07-06T13:11:36.284742Z","iopub.status.idle":"2021-07-06T13:11:38.423387Z","shell.execute_reply.started":"2021-07-06T13:11:36.284706Z","shell.execute_reply":"2021-07-06T13:11:38.422492Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Orange Example","metadata":{}},{"cell_type":"code","source":"orange_images = read_image_folder('../input/small-clip-comparison-dataset/Oranges')\n\norange_images = image_example_producer(orange_images + random_sample(random_noise_images, 2))\n\norange_texts = ['Orange', 'Orange, a type of fruit']\n\norange_texts_tensor = text_example_producer(orange_texts + random_noise_texts)\nplot_similarity_scores(orange_images, orange_texts_tensor, padding = 2)\nplot_matrix(orange_images, orange_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:38.424694Z","iopub.execute_input":"2021-07-06T13:11:38.425056Z","iopub.status.idle":"2021-07-06T13:11:40.451687Z","shell.execute_reply.started":"2021-07-06T13:11:38.425017Z","shell.execute_reply":"2021-07-06T13:11:40.450688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Satellite Images Example","metadata":{}},{"cell_type":"code","source":"satellite_images = read_image_folder('../input/small-clip-comparison-dataset/Satelite')\n\nsatelite_images = image_example_producer(satellite_images)\n\nsatellite_texts = ['Picture of a buckingham palace', 'Satellite picture of a buckingham palace']\nsatellite_texts_tensor = text_example_producer(satellite_texts)\n\nplot_similarity_scores(satelite_images, satellite_texts_tensor)\nplot_matrix(satelite_images, satellite_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:40.453103Z","iopub.execute_input":"2021-07-06T13:11:40.453473Z","iopub.status.idle":"2021-07-06T13:11:41.305163Z","shell.execute_reply.started":"2021-07-06T13:11:40.453420Z","shell.execute_reply":"2021-07-06T13:11:41.304205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vehicle Example ","metadata":{}},{"cell_type":"code","source":"vehicle_images = read_image_folder('../input/small-clip-comparison-dataset/Vehicles')\n\nvehicle_images = image_example_producer(vehicle_images + random_sample(random_noise_images, 2))\n\nboat_texts = ['Yacht', 'Yacht, a type of vehicle', 'Yacht, a type of boat', 'boat', 'rowboat', 'rowboat, a type of boat', 'rowboat, a type of vehicle' ]\ncar_texts = ['Ferrari', 'Ferrari, a type of car', 'Ferrari, a type of vehicle', 'Jetta', 'Jetta, a type of car', 'Jetta, a type of vehicle']\nother_texts = ['Boeing 747', 'Boeing 747, a type of vehicle', 'Boeing 747, a type of airplane', 'Tank', 'Tank, a type of vehicle']\n\nvehicle_texts = car_texts + boat_texts + other_texts\n\nvehicle_texts_tensor = text_example_producer(vehicle_texts)\n\nplot_similarity_scores(vehicle_images, vehicle_texts_tensor)\nplot_matrix(vehicle_images, vehicle_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:41.306657Z","iopub.execute_input":"2021-07-06T13:11:41.307005Z","iopub.status.idle":"2021-07-06T13:11:44.258750Z","shell.execute_reply.started":"2021-07-06T13:11:41.306969Z","shell.execute_reply":"2021-07-06T13:11:44.257768Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Computer and Laptop Example\n","metadata":{}},{"cell_type":"code","source":"pc_and_laptop_images = read_image_folder('../input/small-clip-comparison-dataset/Computers and Laptops')\n\npc_and_laptop_images = image_example_producer(pc_and_laptop_images + random_sample(random_noise_images, 2))\n\npc_and_laptop_texts = ['Macbook', 'Macbook, a type of laptop', 'Macbook, a type of computer', 'Lenovo Ideapad', 'Lenovo Ideapad, a type of laptop', 'Lenovo Ideapad, a type of computer', 'A desktop computer']\npc_and_laptop_texts_tensor = text_example_producer(pc_and_laptop_texts + random_noise_texts)\n\nplot_similarity_scores(pc_and_laptop_images, pc_and_laptop_texts_tensor, padding = 5)\nplot_matrix(pc_and_laptop_images, pc_and_laptop_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:44.260157Z","iopub.execute_input":"2021-07-06T13:11:44.260507Z","iopub.status.idle":"2021-07-06T13:11:46.537991Z","shell.execute_reply.started":"2021-07-06T13:11:44.260463Z","shell.execute_reply":"2021-07-06T13:11:46.537039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Accessory Example","metadata":{}},{"cell_type":"code","source":"accessory_images = read_image_folder('../input/small-clip-comparison-dataset/Accessory')\n\naccessory_images = image_example_producer(accessory_images + random_sample(random_noise_images, 2))\n\naccessory_texts = ['Backpack', 'Backpack, a type of accessory', 'waist bag', 'waist bag, a type of accessory', 'fanny pack', 'fanny pack, a type of accessory', 'belt bag', 'belt bag, a type of accessory', 'Watch', 'Watch, a type of accessory', 'Sunglasses', 'Sunglasses, a type of accessory', ]\naccessory_texts_tensor = text_example_producer(accessory_texts + random_noise_texts)\n\nplot_similarity_scores(accessory_images, accessory_texts_tensor, padding = 2)\nplot_matrix(accessory_images, accessory_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:46.539448Z","iopub.execute_input":"2021-07-06T13:11:46.539789Z","iopub.status.idle":"2021-07-06T13:11:48.855717Z","shell.execute_reply.started":"2021-07-06T13:11:46.539749Z","shell.execute_reply":"2021-07-06T13:11:48.854725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Fannypack Example","metadata":{}},{"cell_type":"code","source":"fannypack_images = read_image_folder('../input/small-clip-comparison-dataset/FannyPack')\n\nfannypack_images = image_example_producer(fannypack_images + random_sample(random_noise_images, 2))\n\nfannypack_texts = ['waist bag', 'waist bag, a type of accessory', 'fanny pack', 'fanny pack, a type of accessory', 'belt bag', 'belt bag, a type of accessory', 'Zelda, a type of video game']\nfannypack_texts_tensor = text_example_producer(fannypack_texts + random_noise_texts)\n\nplot_similarity_scores(fannypack_images, fannypack_texts_tensor, padding = 2)\nplot_matrix(fannypack_images, fannypack_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:48.857169Z","iopub.execute_input":"2021-07-06T13:11:48.857525Z","iopub.status.idle":"2021-07-06T13:11:51.367118Z","shell.execute_reply.started":"2021-07-06T13:11:48.857490Z","shell.execute_reply":"2021-07-06T13:11:51.366139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Food Example","metadata":{}},{"cell_type":"code","source":"food_images = read_image_folder('../input/small-clip-comparison-dataset/Food')\n\nfood_images = image_example_producer(food_images + random_sample(random_noise_images, 2))\n\nfood_texts = ['Candy Apple', 'Candy Apple, a type of food', 'Pizza', 'Pizza, a type of food', 'Cake', 'Cake, a type of food', 'Hotdog', 'Hotdog, a type of food']\nfood_texts_tensor = text_example_producer(food_texts + random_noise_texts)\n\nplot_similarity_scores(food_images, food_texts_tensor, padding = 2)\nplot_matrix(food_images, food_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:51.368571Z","iopub.execute_input":"2021-07-06T13:11:51.368907Z","iopub.status.idle":"2021-07-06T13:11:53.836282Z","shell.execute_reply.started":"2021-07-06T13:11:51.368870Z","shell.execute_reply":"2021-07-06T13:11:53.835341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## People and Action Figures Example","metadata":{}},{"cell_type":"code","source":"people_and_action_figures_images = read_image_folder('../input/small-clip-comparison-dataset/People and action figures')\n\npeople_and_action_figures_images = image_example_producer(people_and_action_figures_images + random_sample(random_noise_images, 2))\n\npeople_and_action_figures_texts = ['Superman', 'Superman, an action figure', 'Ironman', 'Ironman, an action figure', 'Man', 'Man, a type of human', 'Woman', 'Woman, a type of human']\npeople_and_action_figures_texts_tensor = text_example_producer(people_and_action_figures_texts + random_noise_texts)\n\nplot_similarity_scores(people_and_action_figures_images, people_and_action_figures_texts_tensor, padding = 2.0)\nplot_matrix(people_and_action_figures_images, people_and_action_figures_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:53.837721Z","iopub.execute_input":"2021-07-06T13:11:53.838133Z","iopub.status.idle":"2021-07-06T13:11:56.649402Z","shell.execute_reply.started":"2021-07-06T13:11:53.838097Z","shell.execute_reply":"2021-07-06T13:11:56.648455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weapons Example","metadata":{}},{"cell_type":"code","source":"weapons_images = read_image_folder('../input/small-clip-comparison-dataset/Weapons')\n\nweapons_images = image_example_producer(weapons_images + random_sample(random_noise_images, 2))\n\nweapons_texts = ['Pistol', 'Pistol, a type of weapon', 'Rifle', 'Rifle, a type of weapon', 'Nerf gun', 'Nerf gun, a type of toy']\nweapons_texts_tensor = text_example_producer(weapons_texts + random_noise_texts)\n\nplot_similarity_scores(weapons_images, weapons_texts_tensor, padding = 2.0)\nplot_matrix(weapons_images, weapons_texts_tensor)","metadata":{"execution":{"iopub.status.busy":"2021-07-06T13:11:56.650872Z","iopub.execute_input":"2021-07-06T13:11:56.651224Z","iopub.status.idle":"2021-07-06T13:11:58.671478Z","shell.execute_reply.started":"2021-07-06T13:11:56.651189Z","shell.execute_reply":"2021-07-06T13:11:58.670666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}